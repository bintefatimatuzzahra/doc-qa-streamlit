{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOIWMq38+CRXLdVduM+L7NZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Final Cleaned Up code"],"metadata":{"id":"YofzI5Xhhbru"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uM6ZQ25Kj5v-","executionInfo":{"status":"ok","timestamp":1734375533828,"user_tz":-300,"elapsed":4574,"user":{"displayName":"Binte Fatima Tuz Zahra","userId":"16463446727988883636"}},"outputId":"45028208-e7ff-490e-c5c9-cae6f66c2a89"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip install -r /content/drive/MyDrive/bintefatimatuzzahra28/ML_Project/requirements.txt -q"],"metadata":{"id":"Tqmf2BsEkDeO","executionInfo":{"status":"ok","timestamp":1734375613716,"user_tz":-300,"elapsed":6249,"user":{"displayName":"Binte Fatima Tuz Zahra","userId":"16463446727988883636"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["import getpass\n","import os\n","\n","if not os.getenv(\"COHERE_API_KEY\"):\n","    os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Enter your Cohere API key: \")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wJlXjR4gkHGc","executionInfo":{"status":"ok","timestamp":1734375704842,"user_tz":-300,"elapsed":18228,"user":{"displayName":"Binte Fatima Tuz Zahra","userId":"16463446727988883636"}},"outputId":"471a4b28-4285-4b43-8ca5-c175b48b215d"},"execution_count":3,"outputs":[{"name":"stdout","output_type":"stream","text":["Enter your Cohere API key: ··········\n"]}]},{"cell_type":"code","source":["import getpass\n","import os\n","\n","if not os.getenv(\"GoogleVertex_API_KEY\"):\n","    os.environ[\"GoogleVertex_API_KEY\"] = getpass.getpass(\"Enter your Google Vertex API key: \")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ifra_5s4kIA2","executionInfo":{"status":"ok","timestamp":1734375713735,"user_tz":-300,"elapsed":6709,"user":{"displayName":"Binte Fatima Tuz Zahra","userId":"16463446727988883636"}},"outputId":"d3fa5b12-bb80-4afe-93b5-32acf2f97c58"},"execution_count":4,"outputs":[{"name":"stdout","output_type":"stream","text":["Enter your Google Vertex API key: ··········\n"]}]},{"cell_type":"code","source":["os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/drive/MyDrive/bintefatimatuzzahra28/ML_Project/mymlproject-444721-140c4261cfb8.json\""],"metadata":{"id":"33XYiOWGkKvw","executionInfo":{"status":"ok","timestamp":1734375719010,"user_tz":-300,"elapsed":368,"user":{"displayName":"Binte Fatima Tuz Zahra","userId":"16463446727988883636"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["import os\n","import json\n","import numpy as np\n","import faiss\n","import streamlit as st\n","from langchain.document_loaders import PyPDFLoader, TextLoader\n","from tempfile import NamedTemporaryFile\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain_cohere import CohereEmbeddings\n","from langchain.prompts import PromptTemplate\n","from langchain.chains import LLMChain\n","from langchain_google_vertexai import ChatVertexAI\n","from tenacity import retry, stop_after_attempt, wait_exponential\n","from pyngrok import ngrok\n","\n","ngrok.set_auth_token(\"2po1TCRu94I8GFTrAfJ4wkhmbKj_7CkEpn4edG2oAWV2t9zQa\")\n","\n","# Token estimation function\n","def estimate_tokens(text):\n","    words = text.split()\n","    return int(len(words) / 0.75)  # Rough token estimate per document\n","\n","\n","# Process Uploaded Files\n","def process_uploaded_files(uploaded_files):\n","    \"\"\"Process and load documents from uploaded files.\"\"\"\n","    document_list = []\n","    for uploaded_file in uploaded_files:\n","        # Save the uploaded file to a temporary file\n","        file_extension = os.path.splitext(uploaded_file.name)[1].lower()\n","\n","        with NamedTemporaryFile(delete=False, suffix=file_extension) as tmp_file:\n","            tmp_file.write(uploaded_file.getvalue())\n","            temp_file_path = tmp_file.name\n","\n","        # Choose loader based on file type\n","        if file_extension == \".pdf\":\n","            loader = PyPDFLoader(temp_file_path)\n","        elif file_extension == \".txt\":\n","            loader = TextLoader(temp_file_path, encoding=\"utf-8\")  # Use UTF-8 encoding for text files\n","        else:\n","            st.warning(f\"Unsupported file format: {file_extension}. Skipping {uploaded_file.name}\")\n","            continue\n","\n","        # Load documents and extend the document list\n","        try:\n","            documents = loader.load()\n","            document_list.extend(documents)\n","        except Exception as e:\n","            st.error(f\"Error loading {uploaded_file.name}: {e}\")\n","            continue\n","\n","        # Optionally clean up the temporary file after processing\n","        os.remove(temp_file_path)\n","\n","    return document_list\n","\n","\n","# Split Documents into Chunks\n","def split_docs(documents, chunk_size=1000, chunk_overlap=20, max_tokens=2000):\n","    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n","    chunks = text_splitter.split_documents(documents)\n","\n","    # Ensure chunks do not exceed token limit\n","    valid_chunks = []\n","    for chunk in chunks:\n","        token_count = estimate_tokens(chunk.page_content)\n","        if token_count <= max_tokens:\n","            valid_chunks.append(chunk)\n","        else:\n","            st.warning(f\"Skipping chunk due to token limit: {chunk.page_content[:100]}... (tokens: {token_count})\")\n","\n","    return valid_chunks\n","\n","\n","# Create the Cohere Embedding Model\n","embeddings = CohereEmbeddings(model=\"embed-english-v3.0\")\n","\n","\n","# Implement Retry Logic for Embeddings\n","@retry(stop=stop_after_attempt(5), wait=wait_exponential(min=1, max=10))\n","def embed_documents_with_retry(texts, embeddings):\n","    return embeddings.embed_documents(texts)\n","\n","\n","# Create FAISS Vectorstore from Docs\n","def create_faiss_vectorstore_from_docs(docs, embeddings, faiss_index_path, metadata_path):\n","    texts = [doc.page_content for doc in docs]\n","\n","    # Batch embedding to prevent rate limit issues\n","    embeddings_matrix = []\n","    batch_size = 100  # Adjust based on rate limit\n","    for i in range(0, len(texts), batch_size):\n","        batch = texts[i:i + batch_size]\n","        embeddings_matrix.extend(embed_documents_with_retry(batch, embeddings))\n","\n","    embeddings_array = np.array(embeddings_matrix).astype('float32')\n","    dimension = embeddings_array.shape[1]\n","    index = faiss.IndexFlatL2(dimension)\n","    index.add(embeddings_array)\n","\n","    # Overwrite FAISS index if it exists\n","    if os.path.exists(faiss_index_path):\n","        os.remove(faiss_index_path)\n","    faiss.write_index(index, faiss_index_path)\n","\n","    # Save updated metadata\n","    metadata = [{'doc_id': i, 'content': doc.page_content} for i, doc in enumerate(docs)]\n","    with open(metadata_path, 'w') as f:\n","        json.dump(metadata, f)\n","\n","    return index\n","\n","\n","# Perform Retrieval and Generate Answer\n","def search_similar_docs_with_faiss_and_generate_answer(\n","    query, index, metadata, embeddings, model, k=3, max_context_tokens=2000\n","):\n","    \"\"\"Retrieve and synthesize information from multiple documents, prioritizing by relevance score.\"\"\"\n","    query_embedding = embeddings.embed_query(query)\n","    query_embedding = np.array(query_embedding).astype('float32').reshape(1, -1)\n","\n","    # Perform FAISS search\n","    distances, indices = index.search(query_embedding, k)\n","\n","    if indices[0].size > 0 and np.any(indices[0] != -1):  # Ensure valid retrieval\n","        # Pair retrieved document indices with their relevance scores (distances)\n","        retrieved_docs = []\n","        for idx, score in zip(indices[0], distances[0]):\n","            if idx != -1:\n","                retrieved_docs.append((metadata[idx], score))\n","\n","        # Deduplicate the retrieved documents\n","        unique_docs = {doc['content']: (doc, score) for doc, score in retrieved_docs}.values()\n","        retrieved_docs = list(unique_docs)\n","\n","        # Sort documents by relevance (ascending distance means higher relevance)\n","        retrieved_docs.sort(key=lambda x: x[1])\n","\n","        st.write(f\"Retrieved {len(retrieved_docs)} document(s) for the query, sorted by relevance:\")\n","        #for i, (doc, score) in enumerate(retrieved_docs):  # Display snippets and scores\n","        #    st.write(f\"Document {i + 1} | Score: {score:.4f} | Snippet: {doc['content'][:200]}...\")\n","\n","        # Concatenate the most relevant context for the LLM\n","        context = \"\\n---\\n\".join(\n","            [f\"Document {i + 1} (Score: {score:.4f}):\\n{doc['content']}\" for i, (doc, score) in enumerate(retrieved_docs)]\n","        )\n","\n","        # Summarize context if it exceeds token limit\n","        if len(context.split()) > max_context_tokens:\n","            st.write(\"Context too large; summarizing the top documents...\")\n","            context = summarize_long_context([doc for doc, _ in retrieved_docs], model, max_context_tokens)\n","\n","        if not context.strip():\n","            st.write(\"No sufficient context retrieved to answer the query.\")\n","            return \"I cannot determine this from the provided information.\"\n","\n","        # Generate the answer\n","        # st.write(f\"Context passed to LLM: {context[:500]}...\")  # Debugging output\n","        answer = generate_answer_with_llm(query, [doc for doc, _ in retrieved_docs], model)\n","        st.write(f\"### Generated Answer: {answer}\")\n","        return answer\n","    else:\n","        st.write(\"No relevant documents found for the query.\")\n","        return \"I cannot determine this from the provided information.\"\n","\n","\n","\n","\n","# Generate Answer Using LLM\n","def generate_answer_with_llm(query, retrieved_docs, model):\n","    context = \"\\n---\\n\".join([f\"Document {i+1}:\\n{doc['content']}\" for i, doc in enumerate(retrieved_docs)])\n","\n","    prompt = PromptTemplate(\n","        input_variables=[\"question\", \"context\"],\n","        template=\"\"\"You are a helpful and advanced assistant designed to process information from provided documents and you can reason across multiple documents.\n","                    Your task is to answer the question based on the provided documents. If the answer cannot be determined from the documents, respond with \"I cannot determine this from the provided information.\"\n","\n","        Context:\n","        {context}\n","\n","        Question:\n","        {question}\n","\n","        Answer:\"\"\"\n","    )\n","\n","    llm_chain = LLMChain(prompt=prompt, llm=model)\n","    answer = llm_chain.run({\"question\": query, \"context\": context})\n","    return answer\n","\n","\n","# Summarize Long Context to Fit Token Limit\n","def summarize_long_context(retrieved_docs, model, max_context_tokens=2000):\n","    \"\"\"Summarize documents to fit within the token limit.\"\"\"\n","    summaries = []\n","    prompt = PromptTemplate(\n","        input_variables=[\"content\"],\n","        template=\"\"\"Summarize the following document to capture the key points in 200 words or less:\n","\n","        {content}\n","\n","        Summary:\"\"\"\n","    )\n","\n","    llm_chain = LLMChain(prompt=prompt, llm=model)\n","\n","    for doc in retrieved_docs:\n","        summary = llm_chain.run({\"content\": doc['content']})\n","        summaries.append(summary.strip())\n","\n","    # Combine summaries, ensuring they fit within the token limit\n","    combined_summary = \"\\n---\\n\".join(summaries)\n","    return combined_summary[:max_context_tokens]\n","\n","# Process Uploaded Files and Update FAISS Index\n","def update_index_with_new_files(uploaded_files, faiss_index_path, metadata_path, embeddings):\n","    if uploaded_files:\n","        # Process newly uploaded documents\n","        new_docs = process_uploaded_files(uploaded_files)\n","        new_docs = split_docs(new_docs, chunk_size=1000, chunk_overlap=20, max_tokens=2000)\n","\n","        if new_docs:\n","            # Load existing index and metadata if they exist\n","            if os.path.exists(faiss_index_path) and os.path.exists(metadata_path):\n","                st.write(\"Updating existing FAISS index...\")\n","                existing_index = faiss.read_index(faiss_index_path)\n","                with open(metadata_path, 'r') as f:\n","                    existing_metadata = json.load(f)\n","            else:\n","                st.write(\"Creating a new FAISS index...\")\n","                existing_index = None\n","                existing_metadata = []\n","\n","            # Embed new documents\n","            new_texts = [doc.page_content for doc in new_docs]\n","            new_embeddings = []\n","            for i in range(0, len(new_texts), 100):  # Batch embedding\n","                batch = new_texts[i:i + 100]\n","                new_embeddings.extend(embed_documents_with_retry(batch, embeddings))\n","\n","            new_embeddings_array = np.array(new_embeddings).astype('float32')\n","\n","            # Create or update FAISS index\n","            if existing_index:\n","                existing_index.add(new_embeddings_array)\n","                metadata = existing_metadata + [{'doc_id': len(existing_metadata) + i, 'content': doc.page_content}\n","                                                 for i, doc in enumerate(new_docs)]\n","            else:\n","                dimension = new_embeddings_array.shape[1]\n","                existing_index = faiss.IndexFlatL2(dimension)\n","                existing_index.add(new_embeddings_array)\n","                metadata = [{'doc_id': i, 'content': doc.page_content} for i, doc in enumerate(new_docs)]\n","\n","            # Save the updated FAISS index and metadata\n","            faiss.write_index(existing_index, faiss_index_path)\n","            with open(metadata_path, 'w') as f:\n","                json.dump(metadata, f)\n","\n","            return existing_index, metadata\n","    return None, None\n","\n","\n","# Load FAISS Index and Metadata\n","def load_faiss_index(faiss_index_path):\n","    try:\n","        index = faiss.read_index(faiss_index_path)\n","        return index\n","    except Exception as e:\n","        st.write(f\"Error loading FAISS index: {e}\")\n","        return None\n","\n","def load_metadata(metadata_path):\n","    try:\n","        with open(metadata_path, 'r') as f:\n","            metadata = json.load(f)\n","        return metadata\n","    except Exception as e:\n","        st.write(f\"Error loading metadata: {e}\")\n","        return []\n","\n","\n","# Main Streamlit Interface\n","def main():\n","    st.title(\"Document-based Question Answering System\")\n","\n","    # Upload Documents\n","    uploaded_files = st.file_uploader(\n","        \"Upload your documents (.pdf or .txt):\",\n","        type=[\"pdf\", \"txt\"],\n","        accept_multiple_files=True,\n","    )\n","\n","    # FAISS index and metadata paths (hidden)\n","    faiss_index_path = \"/content/drive/MyDrive/bintefatimatuzzahra28/ML_Project/faiss_metadata/faiss_index.index\"\n","    metadata_path = \"/content/drive/MyDrive/bintefatimatuzzahra28/ML_Project/faiss_metadata/metadata.json\"\n","\n","    # Initialize Vertex AI model\n","    model = ChatVertexAI(model=\"gemini-1.5-flash\", project_id=\"mymlproject-444721\")\n","\n","    # Check if FAISS index exists and load or create\n","    if os.path.exists(faiss_index_path) and os.path.exists(metadata_path):\n","        st.write(\"FAISS index exists. Loading FAISS index...\")\n","        metadata = load_metadata(metadata_path)\n","        index = load_faiss_index(faiss_index_path)\n","    else:\n","        st.write(\"No FAISS index found. Please upload documents to create the index.\")\n","        metadata, index = None, None\n","\n","    # Update FAISS index and metadata if new files are uploaded\n","    new_index, new_metadata = update_index_with_new_files(\n","        uploaded_files, faiss_index_path, metadata_path, embeddings\n","    )\n","    if new_index and new_metadata:\n","        index = new_index\n","        metadata = new_metadata\n","\n","    # Query System\n","    query = st.text_input(\"Enter your question:\")\n","\n","    if query:\n","        if index and metadata:\n","            search_similar_docs_with_faiss_and_generate_answer(query, index, metadata, embeddings, model)\n","        else:\n","            st.write(\"No documents available for querying. Please upload files.\")\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B7iaQzvgj3G-","executionInfo":{"status":"ok","timestamp":1734375739079,"user_tz":-300,"elapsed":8506,"user":{"displayName":"Binte Fatima Tuz Zahra","userId":"16463446727988883636"}},"outputId":"280e0f3e-4ece-4713-be14-aa03d086d86b"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["2024-12-16 19:02:14.090 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n","2024-12-16 19:02:14.466 \n","  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n","  command:\n","\n","    streamlit run /usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n","2024-12-16 19:02:14.468 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n","2024-12-16 19:02:14.473 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n","2024-12-16 19:02:14.476 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n","2024-12-16 19:02:14.477 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n","2024-12-16 19:02:14.480 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n","2024-12-16 19:02:14.481 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n","2024-12-16 19:02:14.549 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n","2024-12-16 19:02:14.551 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n","2024-12-16 19:02:14.554 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n","2024-12-16 19:02:14.556 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n","2024-12-16 19:02:15.641 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n","2024-12-16 19:02:15.643 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n","2024-12-16 19:02:15.644 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n","2024-12-16 19:02:15.650 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n","2024-12-16 19:02:15.652 Session state does not function when running a script without `streamlit run`\n","2024-12-16 19:02:15.653 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n","2024-12-16 19:02:15.654 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"]}]},{"cell_type":"code","source":["%cd \"/content/drive/MyDrive/bintefatimatuzzahra28/ML_Project/\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DAgbp8uBkZ-r","executionInfo":{"status":"ok","timestamp":1734375794064,"user_tz":-300,"elapsed":405,"user":{"displayName":"Binte Fatima Tuz Zahra","userId":"16463446727988883636"}},"outputId":"7ad03b5f-bc9d-46a4-a750-ee06acc95e8d"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/bintefatimatuzzahra28/ML_Project\n"]}]},{"cell_type":"code","source":["from pyngrok import ngrok\n","\n","# Start Streamlit in the background\n","!streamlit run copyofFinalizedCode.py &>/dev/null &\n","#!streamlit run txtloader_w_new_prompt.py &>/dev/null &\n","#!streamlit run New_w_txtloader.py &>/dev/null &\n","#!streamlit run metadata_faiss_index_fixed_wsettingsapply.py &>/dev/null &\n","#!streamlit run app.py &>/dev/null &\n","#!streamlit run app.py\n","\n","# Set up an ngrok tunnel to the Streamlit app\n","public_url = ngrok.connect(8501)\n","print(f\"Streamlit app is live at: {public_url}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d5_EzLkukc5V","executionInfo":{"status":"ok","timestamp":1734376824321,"user_tz":-300,"elapsed":772,"user":{"displayName":"Binte Fatima Tuz Zahra","userId":"16463446727988883636"}},"outputId":"084f58c5-2b0c-440c-b116-56b2ab16a432"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Streamlit app is live at: NgrokTunnel: \"https://2d23-34-106-252-26.ngrok-free.app\" -> \"http://localhost:8501\"\n"]}]},{"cell_type":"code","source":["!lsof -i:8501"],"metadata":{"id":"_KLQuFMnkdt4","executionInfo":{"status":"ok","timestamp":1734376805514,"user_tz":-300,"elapsed":387,"user":{"displayName":"Binte Fatima Tuz Zahra","userId":"16463446727988883636"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"88154d9b-799d-4f1f-83e9-f868358eccdf"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["COMMAND     PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME\n","streamlit 29903 root    6u  IPv4 884276      0t0  TCP *:8501 (LISTEN)\n","streamlit 29903 root    7u  IPv6 884277      0t0  TCP *:8501 (LISTEN)\n"]}]},{"cell_type":"code","source":["!kill -9 29903"],"metadata":{"id":"E3gf_432kgxT","executionInfo":{"status":"ok","timestamp":1734376815273,"user_tz":-300,"elapsed":373,"user":{"displayName":"Binte Fatima Tuz Zahra","userId":"16463446727988883636"}}},"execution_count":12,"outputs":[]}]}